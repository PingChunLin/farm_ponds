{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Farm ponds identification pipeline: Network Selection\n",
    "\n",
    "**Make sure you have tiles of the satellite images and corresponding masks in the corresponding files in the training and validation folders.** The tiles and the mask should follow the naming scheme: ```tile_x_y.png```, where x is the top left coordinates ```(x,y)``` of the image (in pixles). If you don't have a set of images in the train and val files, or do not have the COCO JSON files, please visit the [generate-trainning notebook](../0_generate-training/generate-training.ipynb)\n",
    "\n",
    "## Install the packages for the pipeline\n",
    "Make sure you have the environment set up done, so that we can import the packages used in this notebook. Check out the [setup info](../../README.md). Traning the instance segmentation model requires ```torch``` and ```detectron2```, so here we need to install the libraries first. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries that are used in the pipeline\n",
    "Part of the libraries and requirements used in this pipeline are based on Detectron2 documentation. You can find more details of the pre-train models from [Meta Research's Github repository](https://github.com/facebookresearch/detectron2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 658,
     "status": "ok",
     "timestamp": 1688167702071,
     "user": {
      "displayName": "Ping-Chun Lin",
      "userId": "07833352389779309997"
     },
     "user_tz": 420
    },
    "id": "0d288Z2mF5dC",
    "outputId": "1a93db01-087a-4256-ae2b-c961354fb4d2"
   },
   "outputs": [],
   "source": [
    "import torch, detectron2\n",
    "\n",
    "# Setup detectron2 logger\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "import numpy as np\n",
    "import os, cv2, random, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg, LazyConfig\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.data import DatasetMapper, build_detection_train_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.projects import point_rend # uncomment if pointrend\n",
    "\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up the folder paths and parameters\n",
    "You don't need to change the folder paths as the missing folders should be created if they do not exist. The data produced in the pipeline will be stored in the corresponding folders (e.g. training data in train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "ponds_root = os.path.dirname(os.path.dirname(os.getcwd())) \n",
    "if ponds_root not in sys.path:\n",
    "    sys.path.append(ponds_root)\n",
    "train_image_path = os.path.join(ponds_root, \"data/train.png\")  # Path to the input image\n",
    "train_mask_path =  os.path.join(ponds_root,\"data/train_mask.png\")\n",
    "train_folder =  os.path.join(ponds_root,\"data/train/\")  # Output folder for tiles\n",
    "train_mask_folder =  os.path.join(ponds_root,\"data/train_mask/\")\n",
    "train_not_used_folder =  os.path.join(ponds_root,\"data/train_not_used/\")\n",
    "val_folder =  os.path.join(ponds_root,\"data/val/\")\n",
    "val_mask_folder =   os.path.join(ponds_root,\"data/val_mask/\")\n",
    "output_folder = os.path.join(ponds_root, \"output/\")\n",
    "parameter_folder = os.path.join(output_folder, \"parameters/\")\n",
    "model_folder = os.path.join(output_folder, \"model/\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "b2bjrfb2LDeo"
   },
   "source": [
    "### 2. Resigter custom COCO dataset\n",
    "\n",
    "Import a COCO Json file here for the training and validation data. Our goal here is to identify farm ponds with certain artifacts. So here we register a custom dataset that includes the training and validation image set for farm ponds that was used as an example in [the previous step](../0_generate-training/generate-training.ipynb). \n",
    "\n",
    "To make sure that the code properly registers the instance labels, place the ```train.json``` in ```train```, and ```val.json``` in ```val```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "error",
     "timestamp": 1688172020082,
     "user": {
      "displayName": "Ping-Chun Lin",
      "userId": "07833352389779309997"
     },
     "user_tz": 420
    },
    "id": "PIbAM2pv-urF",
    "outputId": "b7f496ea-4449-4e25-c174-f9e1f00b41d1"
   },
   "outputs": [],
   "source": [
    "register_coco_instances(\"pond_train\", {}, os.path.join(train_folder,\"train.json\"), train_folder)\n",
    "register_coco_instances(\"pond_val\", {}, os.path.join(val_folder,\"val.json\"), val_folder)\n",
    "pond_metadata = MetadataCatalog.get(\"pond_train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6ljbWTX0Wi8E"
   },
   "source": [
    "### (Optional) Visualize masks in the train data\n",
    "\n",
    "To examine the images in the training set, the code below shows three random images from the dataset. You can change the number of images shown by changing the variable ```number_of_images```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1H4cFs2T9nMtJAg3sjh1haPG_LCC_VFJD"
    },
    "executionInfo": {
     "elapsed": 5232,
     "status": "ok",
     "timestamp": 1688172031615,
     "user": {
      "displayName": "Ping-Chun Lin",
      "userId": "07833352389779309997"
     },
     "user_tz": 420
    },
    "id": "UkNbUzUOLYf0",
    "outputId": "c12d3840-739c-4672-f2da-087b739e22da"
   },
   "outputs": [],
   "source": [
    "dataset_dicts = DatasetCatalog.get(\"pond_train\")\n",
    "number_of_images = 3\n",
    "for d in random.sample(dataset_dicts, number_of_images):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=pond_metadata, scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    plt.imshow(cv2.cvtColor(out.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wlqXIXXhW8dA"
   },
   "source": [
    "### 3. Set up training parameters\n",
    "\n",
    "Here we implement a COCO-pretrained ResNet-101 Mask-RCNN model on the pond_train dataset. You can switch this model into other pretrained or custom models from the [model zoo](https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 382236,
     "status": "ok",
     "timestamp": 1630550598204,
     "user": {
      "displayName": "Yuxin Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghg6Opku-wKBp6oDitOS_6gtk0a9fUkG_13sJRp=s64",
      "userId": "04991596853823833398"
     },
     "user_tz": 420
    },
    "id": "7unkuuiqLdqd",
    "outputId": "ba1716cd-3f3b-401d-bae5-8fbbd2199d9c"
   },
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "#point_rend.add_pointrend_config(cfg) # un comment if changeing model to pointrend\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "#cfg.merge_from_file(\"detectron2_repo/projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\") # uncomment if pointrend\n",
    "cfg.DATASETS.TRAIN = (\"pond_train\")\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "#cfg.MODEL.WEIGHTS = \"detectron2://PointRend/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco/164955410/model_final_edd263.pkl\" # uncomment if pointrend\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2  \n",
    "cfg.SOLVER.BASE_LR = 0.0005  \n",
    "cfg.SOLVER.MAX_ITER = 2000    \n",
    "cfg.SOLVER.STEPS = []       \n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  \n",
    "\n",
    "LazyConfig.save(cfg, os.path.join(parameter_folder, \"config\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up multiple configurations to train the model\n",
    "\n",
    "To optimize our model, we can use a random search approach to finetune the model and find the best performing configuration in the validation step. To prepare for that, we can create multiple sets of configurations to test in the next steps.\n",
    "An error message ```ERROR: Unable to serialize the config to yaml.``` will likely show up when you finish creating the files. It shouldn't be an issue since detectron2 can read pkl files when training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone the repo in order to access pre-defined configs in PointRend project\n",
    "!git clone --branch v0.6 https://github.com/facebookresearch/detectron2.git detectron2_repo # uncomment if pointrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from detectron2.projects import point_rend # uncomment if pointrend\n",
    "# Model selection\n",
    "pretrained_model_path = \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"#\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "# hyperparameters\n",
    "lr_range = [0.0001, 0.001]\n",
    "batch_size_range = [2, 16]\n",
    "#\n",
    "num_experiments = 5 # number of models to test \n",
    "\n",
    "for num in range(1, num_experiments+1):\n",
    "    learning_rate = random.uniform(*lr_range)\n",
    "    batch_size = random.randint(*batch_size_range)\n",
    "    cfg = get_cfg()\n",
    "    #point_rend.add_pointrend_config(cfg) # un comment if changeing model to pointrend\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(pretrained_model_path))\n",
    "    #cfg.merge_from_file(\"detectron2_repo/projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\") # uncomment if pointrend\n",
    "    cfg.DATASETS.TRAIN = (\"pond_train\",)\n",
    "    cfg.DATASETS.TEST = ()\n",
    "    cfg.DATALOADER.NUM_WORKERS = batch_size\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(pretrained_model_path)\n",
    "    #cfg.MODEL.WEIGHTS = \"detectron2://PointRend/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco/164955410/model_final_edd263.pkl\" # uncomment if pointrend\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 2  \n",
    "    cfg.SOLVER.BASE_LR = learning_rate\n",
    "    cfg.SOLVER.MAX_ITER = 16000    \n",
    "    cfg.SOLVER.STEPS = []       \n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1      \n",
    "\n",
    "    LazyConfig.save(cfg, os.path.join(parameter_folder, f\"config_{num}\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration for the models are saved as ```config.pkl```s in the [parameter_folder](../../output/parameters). We are now ready to train the neural network in the [next step](../2_train_network/train_network.ipynb). "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5",
     "timestamp": 1688164861181
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "d2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
