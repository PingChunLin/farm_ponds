{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Farm ponds identification pipeline: Validating the results\n",
    "Before you start this notebook, make sure that you have\n",
    "- Pretrained model configuration: ```config.pkl``` in the [parameters folder](../../output/parameters/)\n",
    "- Tiles of the satellite image: ```tile_x_y.png``` in [train](../../data/train/) and [val](../../data/val/) folders\n",
    "- Tiles of the masks: ```tile_x_y.png``` in [train_mask](../../data/train_mask/) and [val_mask](../../data/val_mask/) folders\n",
    "\n",
    " The tiles and the mask should follow the naming scheme: ```tile_x_y.png```, where x is the top left coordinates ```(x,y)``` of the image (in pixles). \n",
    "\n",
    "## Install the packages for the pipeline\n",
    "Make sure you have the environment set up done, so that we can import the packages used in this notebook. Check out the [setup info](../../README.md). Traning the instance segmentation model requires ```torch``` and ```detectron2```, so here we need to install the libraries first. You can skip this step if you already installed ```detectron2``` in the previous steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries that are used in the pipeline\n",
    "Part of the libraries and requirements used in this pipeline are based on Detectron2 documentation. You can find more details of the pre-train models from [Meta Research's Github repository](https://github.com/facebookresearch/detectron2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 658,
     "status": "ok",
     "timestamp": 1688167702071,
     "user": {
      "displayName": "Ping-Chun Lin",
      "userId": "07833352389779309997"
     },
     "user_tz": 420
    },
    "id": "0d288Z2mF5dC",
    "outputId": "1a93db01-087a-4256-ae2b-c961354fb4d2"
   },
   "outputs": [],
   "source": [
    "import torch, detectron2\n",
    "\n",
    "# Setup detectron2 logger\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "import numpy as np\n",
    "import os, json, cv2, random, sys, re\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.data import DatasetMapper, build_detection_train_loader\n",
    "import cloudpickle\n",
    "from detectron2.config import CfgNode\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.projects import point_rend # uncomment if pointrend\n",
    "\n",
    "\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the folder paths and parameters\n",
    "You don't need to change the folder paths as the missing folders should be created if they do not exist. The data produced in the pipeline will be stored in the corresponding folders (e.g. training data in [train](./ponds/data/train/)) Refer to the [ponds README.md](./ponds/README.md) to see the structure of the folders in the package. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "ponds_root = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "if ponds_root not in sys.path:\n",
    "    sys.path.append(ponds_root)\n",
    "train_image_path = os.path.join(ponds_root, \"data/train.png\")  # Path to the input image\n",
    "train_mask_path =  os.path.join(ponds_root,\"data/train_mask.png\")\n",
    "train_folder =  os.path.join(ponds_root,\"data/train/\")  # Output folder for tiles\n",
    "train_mask_folder =  os.path.join(ponds_root,\"data/train_mask/\")\n",
    "train_not_used_folder =  os.path.join(ponds_root,\"data/train_not_used/\")\n",
    "val_folder =  os.path.join(ponds_root,\"data/val/\")\n",
    "val_mask_folder =   os.path.join(ponds_root,\"data/val_mask/\")\n",
    "output_folder = os.path.join(ponds_root, \"output/\")\n",
    "parameter_folder = os.path.join(output_folder, \"parameters/\")\n",
    "model_folder = os.path.join(output_folder, \"model/\")\n",
    "performance_folder = os.path.join(output_folder, \"performance/\")\n",
    "inference_path = os.path.join(performance_folder, \"inference.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import helpers as hp\n",
    "register_coco_instances(\"pond_train\", {}, os.path.join(train_folder,\"train.json\"), train_folder)\n",
    "register_coco_instances(\"pond_val\", {}, os.path.join(val_folder,\"val.json\"), val_folder)\n",
    "pond_metadata = MetadataCatalog.get(\"pond_train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0e4vdDIOXyxF"
   },
   "source": [
    "## Inference & evaluation using the trained model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = os.path.join(parameter_folder, \"config.pkl\")\n",
    "cfg = hp.load_from_cloudpickle(cfg_path)\n",
    "point_rend.add_pointrend_config(cfg) # un comment if changeing model to pointrend\n",
    "cfg.OUTPUT_DIR = model_folder\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model.pth\")  # path to the trained model \n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run multiple validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pth_files(directory):\n",
    "    # This regular expression matches files with the format 'config_{number}.pkl'\n",
    "    pattern = re.compile(r'model_\\d+\\.pth$')\n",
    "    # List all files in the given directory\n",
    "    files = os.listdir(directory)\n",
    "    # Filter files based on the pattern\n",
    "    matched_files = [file for file in files if pattern.match(file)]\n",
    "    # Return the count of matched files\n",
    "    return len(matched_files)\n",
    "\n",
    "\n",
    "count = count_pth_files(model_folder)\n",
    "print(f\"Number of matching .pth files: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(inference_path, \"w\") as file:\n",
    "    for num in range(1, count+1):\n",
    "        cfg_path = os.path.join(parameter_folder, f\"config_{num}.pkl\")\n",
    "        cfg = load_from_cloudpickle(cfg_path)\n",
    "        #point_rend.add_pointrend_config(cfg) # un comment if changeing model to pointrend\n",
    "        cfg.MODEL.WEIGHTS =  os.path.join(model_folder, f\"model_{num}.pth\")  # path to the trained model \n",
    "        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
    "        predictor = DefaultPredictor(cfg)\n",
    "        evaluator = COCOEvaluator(\"pond_val\", output_dir=performance_folder)\n",
    "        val_loader = build_detection_test_loader(cfg, \"pond_val\")\n",
    "        results = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "        file.write(f\"model_{num}.pth\\n\")\n",
    "        file.write(json.dumps(results, indent=4))\n",
    "        file.write(\"end\\n\\n\")\n",
    "        print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the Performance on the validation data\n",
    "\n",
    "Performance evaluation: AP metric implemented in COCO API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = COCOEvaluator(\"pond_val\", output_dir=performance_folder)\n",
    "val_loader = build_detection_test_loader(cfg, \"pond_val\")\n",
    "results = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick the best performance out of the pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model_and_config(filename):\n",
    "    highest_ap = -1\n",
    "    best_model = None\n",
    "    best_config = None\n",
    "\n",
    "    try:\n",
    "        # Open the file for reading\n",
    "        with open(filename, 'r') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        # Regular expression to match each model's data block\n",
    "        pattern = re.compile(r'(model_(\\d+)\\.pth)(.*?)}end', re.DOTALL)\n",
    "        matches = pattern.finditer(content)\n",
    "\n",
    "        # Process each match\n",
    "        for match in matches:\n",
    "            model_name = match.group(1)\n",
    "            model_number = match.group(2)  # This captures the number in 'model_number.pth'\n",
    "            json_data = match.group(3) + '}'\n",
    "\n",
    "            try:\n",
    "                data = json.loads(json_data.strip())\n",
    "                # Extract the segm AP value\n",
    "                segm_ap = data['segm']['AP']\n",
    "                \n",
    "                # Update the best model if the current AP is higher than the highest found so far\n",
    "                if segm_ap > highest_ap:\n",
    "                    highest_ap = segm_ap\n",
    "                    best_model = model_name\n",
    "                    best_config = f\"config_{model_number}.pkl\"  # Construct the config filename\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"JSON decode error in block for {model_name}. Skipping this block.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"The file was not found. Check your file path.\")\n",
    "    except Exception as e:\n",
    "        print(\"An unexpected error occurred:\", str(e))\n",
    "\n",
    "    return best_model, highest_ap, best_config\n",
    "\n",
    "# Specify the path to your text file containing the inference scores\n",
    "best_model, highest_ap, best_config = find_best_model_and_config(inference_path)\n",
    "if best_model:\n",
    "    print(f\"The model with the highest segmentation AP score is {best_model} with an AP of {highest_ap:.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid model data was found.\")\n",
    "\n",
    "best_performance_path = os.path.join(performance_folder, \"best_performance.txt\")\n",
    "with open(best_performance_path, \"w\") as file:\n",
    "    best_model_path = os.path.join(model_folder, best_model)\n",
    "    best_config_path =os.path.join(parameter_folder, best_config)\n",
    "    file.write(f\"{best_model_path}\\n{best_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = os.path.join(parameter_folder, best_config)\n",
    "cfg = hp.load_from_cloudpickle(cfg_path)\n",
    "cfg.MODEL.WEIGHTS =  os.path.join(model_folder, best_model)  # path to the trained model \n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly select three samples to visualize the prediction results in the val folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dicts = DatasetCatalog.get(\"pond_val\")\n",
    "for d in random.sample(list(dataset_dicts), 3):\n",
    "    print(d[\"file_name\"])\n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)\n",
    "    # calculate \n",
    "    masks = outputs['instances'].pred_masks\n",
    "    print(masks.shape)\n",
    "    print(torch.sum(torch.flatten(masks, start_dim=1),dim=1))\n",
    "  \n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=pond_metadata,\n",
    "                   scale=0.5,\n",
    "                   instance_mode=ColorMode.IMAGE_BW   \n",
    "    )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    plt.imshow(out.get_image()[:, :, ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5",
     "timestamp": 1688164861181
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "d2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
