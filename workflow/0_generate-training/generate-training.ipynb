{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Farm ponds identification pipeline: Generate traning data\n",
    "**Before you start this notebook, make sure that you have a satellite image (PNG), and a mask for training. If not, please create a mask for training with the [training setup](./README.md). Make sure to place both the satellite image and mask in the [data folder](../../data/), with names as train.png and train_mask.png**\n",
    "\n",
    "This notebook will guide you step by step on how you can identify instances in satellite imagery using a computer vision algorithms such as RCNNs. We will start by Preprocessing the image into tiles, split them into training and validation datasets and then start the training process. \n",
    "\n",
    "## Install the packages for the pipeline\n",
    "Make sure you have the environment set up done, so that we can import the packages used in this notebook. Check out the [setup info](../README.md)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries that are used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 658,
     "status": "ok",
     "timestamp": 1688167702071,
     "user": {
      "displayName": "Ping-Chun Lin",
      "userId": "07833352389779309997"
     },
     "user_tz": 420
    },
    "id": "0d288Z2mF5dC",
    "outputId": "1a93db01-087a-4256-ae2b-c961354fb4d2"
   },
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "If you have a large satellite image, this will take a few minutes to run. For the traning data for Kadwanchi withe 1024x1024 px tiles, it takes about 15 minutes.\n",
    "\n",
    "### 1. Setting up the folder paths and parameters\n",
    "You don't need to change the folder paths as the missing folders should be created if they do not exist. The data produced in the pipeline will be stored in the corresponding folders (e.g. training data in [train](../../data/train/)). Refer to the [repository README](../../README.md) to see the structure of the folders in the repository. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ponds_root = os.path.dirname(os.path.dirname(os.getcwd())) \n",
    "if ponds_root not in sys.path:\n",
    "    sys.path.append(ponds_root)\n",
    "\n",
    "# Configuration settings to set up folders \n",
    "train_image_path = os.path.join(ponds_root, \"data/train.png\")  # Path to the input image\n",
    "train_mask_path =  os.path.join(ponds_root,\"data/train_mask.png\")\n",
    "train_folder =  os.path.join(ponds_root,\"data/train/\")  # Output folder for tiles\n",
    "train_mask_folder =  os.path.join(ponds_root,\"data/train_mask/\")\n",
    "train_not_used_folder =  os.path.join(ponds_root,\"data/train_not_used/\")\n",
    "val_folder =  os.path.join(ponds_root,\"data/val/\")\n",
    "val_mask_folder =   os.path.join(ponds_root,\"data/val_mask/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set tile parameters:\n",
    "\n",
    "We create traning data by cutting the satellite image into tiles. Set the ```tile_width``` and ```tile_height``` to values that you prefer. We use 1024x1024 px in the farm ponds example. Set the ```min_mask_size``` to filter out blank tiles (the size of a blank tile in our farm ponds example is below 6496 bytes in an 1024px image).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_width, tile_height = 1024, 1024  # Tile dimensions\n",
    "min_mask_size = 6496  # Minimum tile file size in bytes\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing the tiles \n",
    "This next cell does the following things: \n",
    "1. Divides image and mask into tiles and place them in the traning folder.\n",
    "2. filter the tiles by size, if it is blank or contains little data, filter out the tile.\n",
    "3. Keep training data so that the images data matches the masks. Making sure that all images have a corresponding mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import preprocess as prep\n",
    "# Step 1: Divide the input image into smaller tiles\n",
    "prep.divide_and_save_image(train_image_path, train_folder, tile_width, tile_height)\n",
    "prep.divide_and_save_image(train_mask_path, train_mask_folder, tile_width, tile_height)\n",
    "\n",
    "# Step 2: Filter the masks generated tiles by their file size (filters out irrelevant blank tiles)\n",
    "prep.filter_tiles_by_size(train_mask_folder, min_mask_size) \n",
    "# Step 3: Keep training data so that the images data matches the masks \n",
    "#         Making sure that all images have a corresponding mask\n",
    "prep.process_training_data(train_folder, train_mask_folder, train_not_used_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Splitting the data (both images and masks)\n",
    "\n",
    "Choose the number of random images you want to exclude from training and place in the validation set to validate the performance of the model so that the model we train later does not pick up trends that are too specific to the training data (lower variance, i.e. we want the model to pick up the general trend for farmponds). \n",
    "\n",
    "Here we set the number of images in the validation set to be **150**. Which is roughly about 20% of the total data. You can follow the 80-20 rule, where a randomly chosen 20% of the data becomes validation data. You can also use other rules to determine how you want to validate model's training performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_to_select = 150\n",
    "prep.create_validation_set(train_folder, train_mask_folder, val_folder, val_mask_folder, num_images_to_select)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Inverting image colors for mask (both train and val)\n",
    "Once the train-val split is done, we turn the instances white, and make the background black, so the next cell can recognize the instances when creating a JSON file to descirbe the instances. When finish running the cell below, we are ready to create label information for the model traning later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.invert_image_colors(train_mask_folder, file_extension=\".png\")\n",
    "prep.invert_image_colors(val_mask_folder, file_extension=\".png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Creating COCO dataset for the labeled pond data\n",
    "This part of the code creates a COCO dataset as a JSON file that will be used for training and validating the model. Both [train](../../data/train/) and [val](../../data/val/) folders will have JSON files that labels the masks on the image tiles. This cell is modified from the repository [image-to-coco-json-converter](https://github.com/chrise96/image-to-coco-json-converter.git). You can also customize the labels and add other ```category_ids``` and ```category_colors```.\n",
    "\n",
    "The train/val dataset for the farmponds example data will take about 7 mins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from utils.create_annotations import *\n",
    "\n",
    "# Label ids of the dataset\n",
    "category_ids = {\n",
    "\n",
    "    \"pond\": 1\n",
    "}\n",
    "\n",
    "# Define which colors match which categories in the images\n",
    "category_colors = {\n",
    "    \"(255, 255, 255)\": 1 # pond\n",
    "}\n",
    "\n",
    "# Define the ids that are a multiplolygon. In our case: wall, roof and sky\n",
    "multipolygon_ids = []\n",
    "\n",
    "# Get \"images\" and \"annotations\" info \n",
    "def images_annotations_info(maskpath):\n",
    "    # This id will be automatically increased as we go\n",
    "    annotation_id = 0\n",
    "    image_id = 0\n",
    "    annotations = []\n",
    "    images = []\n",
    "    \n",
    "    for mask_image in glob.glob(maskpath + \"*.png\"):\n",
    "        # The mask image is *.png but the original image is *.jpg.\n",
    "        # We make a reference to the original file in the COCO JSON file\n",
    "        original_file_name = os.path.basename(mask_image).split(\".\")[0] + \".png\"\n",
    "        #print(original_file_name)\n",
    "        # Open the image and (to be sure) we convert it to RGB\n",
    "        mask_image_open = Image.open(mask_image).convert(\"RGB\")\n",
    "        w, h = mask_image_open.size\n",
    "        \n",
    "        # \"images\" info \n",
    "        image = create_image_annotation(original_file_name, w, h, image_id)\n",
    "        images.append(image)\n",
    "\n",
    "        sub_masks = create_sub_masks(mask_image_open, w, h)\n",
    "        for color, sub_mask in sub_masks.items():\n",
    "            category_id = category_colors[color]\n",
    "\n",
    "            # \"annotations\" info\n",
    "            polygons, segmentations = create_sub_mask_annotation(sub_mask)\n",
    "\n",
    "            # Check if we have classes that are a multipolygon\n",
    "            if category_id in multipolygon_ids:\n",
    "                # Combine the polygons to calculate the bounding box and area\n",
    "                multi_poly = MultiPolygon(polygons)\n",
    "                                \n",
    "                annotation = create_annotation_format(multi_poly, segmentations, image_id, category_id, annotation_id)\n",
    "\n",
    "                annotations.append(annotation)\n",
    "                annotation_id += 1\n",
    "            else:\n",
    "                for i in range(len(polygons)):\n",
    "                    # Cleaner to recalculate this variable\n",
    "                    segmentation = [np.array(polygons[i].exterior.coords).ravel().tolist()]\n",
    "                    \n",
    "                    annotation = create_annotation_format(polygons[i], segmentation, image_id, category_id, annotation_id)\n",
    "                    \n",
    "                    annotations.append(annotation)\n",
    "                    annotation_id += 1\n",
    "        image_id += 1\n",
    "    return images, annotations, annotation_id\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get the standard COCO JSON format\n",
    "    coco_format = get_coco_json_format()\n",
    "    \n",
    "    for keyword in [\"train\", \"val\"]:\n",
    "        mask_path = os.path.join(ponds_root, f\"data/{keyword}_mask/\")\n",
    "        # Create category section\n",
    "        coco_format[\"categories\"] = create_category_annotation(category_ids)\n",
    "    \n",
    "        # Create images and annotations sections\n",
    "        coco_format[\"images\"], coco_format[\"annotations\"], annotation_cnt = images_annotations_info(mask_path)\n",
    "        json_path = os.path.join(ponds_root, f\"data/{keyword}/{keyword}.json\")\n",
    "        with open(json_path,\"w\") as outfile:\n",
    "            print(json_path)\n",
    "            json.dump(coco_format, outfile)\n",
    "            \n",
    "        print(\"Created %d annotations for images in folder: %s\" % (annotation_cnt, mask_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the JSON files created in the train/val folders, we are now ready to proceed to the next step: [network_selection](../1_network_selection/network_selection.ipynb). We will also visualize the masks so you can see how the labeled instances look like on the images."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5",
     "timestamp": 1688164861181
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "ponds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
